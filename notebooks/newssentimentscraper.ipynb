{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-07T02:07:40.602914Z",
     "start_time": "2025-06-07T02:07:40.596013Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\saysa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T21:32:59.999857Z",
     "start_time": "2025-06-21T21:32:52.668645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime, time, random, urllib.parse, requests, feedparser, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from email.utils import parsedate_to_datetime\n",
    "\n",
    "BASE = \"https://news.google.com/rss/search\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6_1) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/125.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"application/rss+xml, text/xml;q=0.9\",\n",
    "}\n",
    "\n",
    "def month_slices(start, end):\n",
    "    d = datetime.date(start.year, start.month, 1)\n",
    "    while d <= end:\n",
    "        nxt = (d.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "        yield d, min(nxt - datetime.timedelta(days=1), end)\n",
    "        d = nxt\n",
    "\n",
    "def build_url(keywords, since, until):\n",
    "    q = f'{keywords} after:{since} before:{(until + datetime.timedelta(days=1))}'\n",
    "    params = {\"q\": q, \"hl\": \"en-US\", \"gl\": \"US\", \"ceid\": \"US:en\"}\n",
    "    return f\"{BASE}?{urllib.parse.urlencode(params, quote_via=urllib.parse.quote_plus)}\"\n",
    "\n",
    "def session(max_retries=5):\n",
    "    s = requests.Session()\n",
    "    rs = Retry(total=max_retries, backoff_factor=1,\n",
    "               status_forcelist=[429, 500, 502, 503, 504])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=rs))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def fetch_month(url, sess):\n",
    "    for _ in range(2):                          # up to 1 retry on 503\n",
    "        r = sess.get(url, timeout=15)\n",
    "        if r.status_code == 200:\n",
    "            return feedparser.parse(r.text).entries\n",
    "        if r.status_code == 503:\n",
    "            time.sleep(60)                      # hard back-off\n",
    "    raise RuntimeError(f\"Still 503 → {url}\")\n",
    "\n",
    "def scrape(keywords, start, end, outfile):\n",
    "    sia, rows, sess = SentimentIntensityAnalyzer(), [], session()\n",
    "    for since, until in month_slices(start, end):\n",
    "        url = build_url(keywords, since, until)\n",
    "        for e in fetch_month(url, sess):\n",
    "            score = sia.polarity_scores(e.title)\n",
    "            rows.append({\n",
    "                \"date\": parsedate_to_datetime(e.published).strftime(\"%Y-%m-%d\"),\n",
    "                \"headline\": e.title, \n",
    "                **score\n",
    "            })\n",
    "        time.sleep(random.uniform(6, 12))       # polite spacing\n",
    "    pd.DataFrame(rows).to_csv(outfile, index=False)\n",
    "    print(f\"Saved {len(rows)} rows → {outfile}\")\n",
    "\n",
    "# -------- run --------\n",
    "ipo  = datetime.date(2025, 6, 23)\n",
    "today = datetime.date.today()\n",
    "scrape(\"RBLX stock\", ipo, today, \"../data/roblox_stock_sentiment.csv\")\n"
   ],
   "id": "3e6a665f5569860b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 62 rows → ../data/roblox_stock_sentiment.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T21:41:52.659808Z",
     "start_time": "2025-06-21T21:41:52.651673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def flip_csv_by_date(input_csv, output_csv, date_column='date'):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Convert date column to datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "\n",
    "    # Drop rows where date conversion failed\n",
    "    df = df.dropna(subset=[date_column])\n",
    "\n",
    "    # Sort by date descending\n",
    "    df = df.sort_values(by=date_column, ascending=False)\n",
    "\n",
    "    # Save to new CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Flipped CSV saved to: {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "flip_csv_by_date(\"../data/roblox_stock_sentiment.csv\", \"../data/Sorted_Roblox_Stock_Sentiment.csv\")"
   ],
   "id": "2571cc6b6d0c544d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped CSV saved to: ../data/Sorted_Roblox_Stock_Sentiment.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T01:50:56.116743Z",
     "start_time": "2025-06-09T01:50:56.067482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "dedupe_headlines_fixed.py\n",
    "\n",
    "Just edit the INPUT_CSV and OUTPUT_CSV variables below,\n",
    "then run this script. It will remove duplicate rows based\n",
    "on the 'headline' column (keeping the earliest 'date').\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ─── EDIT THESE ───────────────────────────────────────────\n",
    "INPUT_CSV    = '../data/Merged_Roblox_Sentiment_Data.csv'   # ← change this to your source file\n",
    "OUTPUT_CSV   = 'clean_news_sentiment.csv'  # ← change this to your destination file\n",
    "DATE_COL     = 'date'               # ← name of your date column\n",
    "HEADLINE_COL = 'headline'           # ← name of your headline column\n",
    "# ─────────────────────────────────────────────────────────\n",
    "\n",
    "def remove_duplicates(input_file, output_file,\n",
    "                      date_col=DATE_COL, headline_col=HEADLINE_COL):\n",
    "    # Read CSV, parsing the date column\n",
    "    df = pd.read_csv(input_file, parse_dates=[date_col])\n",
    "\n",
    "    # Sort so earliest dates come first\n",
    "    df_sorted = df.sort_values(by=date_col, ascending=True)\n",
    "\n",
    "    # Drop later duplicates of the same headline\n",
    "    df_deduped = df_sorted.drop_duplicates(subset=[headline_col], keep='first')\n",
    "\n",
    "    # Save result\n",
    "    df_deduped.to_csv(output_file, index=False)\n",
    "    print(f\"Wrote {len(df_deduped)} unique rows to '{output_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    remove_duplicates(INPUT_CSV, OUTPUT_CSV)\n",
    "\n"
   ],
   "id": "f014cbd2e1bfc737",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1930 unique rows to 'deduped_output.csv'\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
